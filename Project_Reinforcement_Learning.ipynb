{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "77333f45",
      "metadata": {
        "id": "77333f45"
      },
      "source": [
        "<a \n",
        " href=\"https://colab.research.google.com/github/LearnPythonWithRune/MachineLearningWithPython/blob/main/colab/final/04 - Project - Reinforcement Learning.ipynb\"\n",
        " target=\"_parent\">\n",
        "<img \n",
        " src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa880c3e",
      "metadata": {
        "id": "fa880c3e"
      },
      "source": [
        "# Project: Reinforcement Learning\n",
        "- Bigger field - more learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39b3c535",
      "metadata": {
        "id": "39b3c535"
      },
      "source": [
        "### Project field\n",
        "- Use Reinforcement Learning with Q-learning to find solutions to this field.\n",
        "![Field](https://raw.githubusercontent.com/LearnPythonWithRune/MachineLearningWithPython/main/jupyter/final/img/field-2.png \"Field\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "effaa2b7",
      "metadata": {
        "id": "effaa2b7"
      },
      "source": [
        "### Step 1: Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dad84ded",
      "metadata": {
        "id": "dad84ded"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ae15067",
      "metadata": {
        "id": "5ae15067"
      },
      "source": [
        "### Step 2: Create a field\n",
        "\n",
        "![Field](img/field-3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ea04310",
      "metadata": {
        "id": "6ea04310"
      },
      "source": [
        "- **__init__**:\n",
        "    - Use a list of list with integer values to represent all the states\n",
        "        - Goal end state should be 1, illegal states -1, other states 0\n",
        "    - Set the state to be random fo the size of states\n",
        "- **done**:\n",
        "    - Check if current state has non-negative values\n",
        "- **get_possible_actions**:\n",
        "    - Set a list to all possible actions **actions = [0, 1, 2, 3]**\n",
        "        - action = 0 is left\n",
        "        - action = 1 is right\n",
        "        - action = 2 is up\n",
        "        - action = 3 is down\n",
        "    - Then check if state is in a position where a possible actions should be removed.\n",
        "    - Finally, return the remaining actions\n",
        "- **update_next_state**:\n",
        "    - Get the current state\n",
        "    - Check if move is illegal, then return current state and -10 in reward\n",
        "    - Otherwise opdate state and return the reward according to new state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04c96397",
      "metadata": {
        "id": "04c96397"
      },
      "outputs": [],
      "source": [
        "class Field:\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"\n",
        "        Initialize field and set a random start state\n",
        "        \"\"\"\n",
        "        self.states = [\n",
        "            [-1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "            [-1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
        "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "            ]\n",
        "        self.state = (random.randrange(0, len(self.states)), random.randrange(0, len(self.states[0])))\n",
        "    \n",
        "    def done(self):\n",
        "        \"\"\"\n",
        "        Check if it isn't in a neutral state\n",
        "        \"\"\"\n",
        "        if self.states[self.state[0]][self.state[1]] != 0:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    def get_possible_actions(self):\n",
        "        \"\"\"\n",
        "        Return possible actions in state\n",
        "\n",
        "        Action:\n",
        "               0 => left\n",
        "               1 => right\n",
        "               2 => up\n",
        "               3 => down\n",
        "        \"\"\"    \n",
        "        actions = [0, 1, 2, 3]\n",
        "        if self.state[0] == 0:\n",
        "            actions.remove(2)\n",
        "        if self.state[0] == len(self.states) -1:\n",
        "            actions.remove(3)\n",
        "        if self.state[1] == 0:\n",
        "            actions.remove(0)\n",
        "        if self.state[1] == len(self.states[0]) -1:\n",
        "            actions.remove(1)\n",
        "        return actions\n",
        "\n",
        "    def update_next_state(self, action):\n",
        "        \"\"\" \n",
        "        Update state according to action -> Return state and reward\n",
        "        \"\"\"\n",
        "        x, y = self.state\n",
        "\n",
        "        if action == 0:\n",
        "            if y == 0:\n",
        "                return self.state, -10\n",
        "            self.state = x, y - 1\n",
        "        if action == 1:\n",
        "            if y == len(self.states) -1:\n",
        "                return self.state, -10\n",
        "            self.state = x, y + 1\n",
        "        if action == 2:\n",
        "            if x == 0:\n",
        "                return self.state, -10\n",
        "            self.state = x - 1, y\n",
        "        if action == 3:\n",
        "            if self.state == len(self.states) -1:\n",
        "                return self.state, -10\n",
        "            self.state = x + 1, y        \n",
        "        reward = self.states[self.state[0]][self.state[1]]\n",
        "        return self.state, reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24a0ca19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24a0ca19",
        "outputId": "be2ce8b1-abec-400c-98b8-f0c4402bd271"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((0, 11), False, [0, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "field = Field()\n",
        "field.state, field.done(), field.get_possible_actions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2fa616f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2fa616f",
        "outputId": "dbe2387a-113f-46f0-ff16-f8d6ad0dd5b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((0, 11), False, [0, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "field.update_next_state(2)\n",
        "field.state, field.done(), field.get_possible_actions()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2cf74c6",
      "metadata": {
        "id": "e2cf74c6"
      },
      "source": [
        "### Step 3: Train the model\n",
        "- Create a $q$-table initialized to all 0\n",
        "    - Use **q_table = np.zeros(...)** *(insert values for ...)*\n",
        "- Set **alpha = .5, gamma = 0.5,** and **epsilon = 0.5**\n",
        "- Create *for*-loop iterating 10000\n",
        "    - Create new field\n",
        "    - While field not done\n",
        "        - Get possible actions and assign to **actions**\n",
        "        - With probability epsilon take a random action, otherwise take the best action\n",
        "            - HINT: **random.uniform(0, 1) < epsilon**\n",
        "            - HINT: Random action: **random.choice(actions)**, and best action: **np.argmax(q_table[field.state])**\n",
        "        - Get current state and assign it to **cur_x, cur_y**\n",
        "        - Update next state and get it and the reward\n",
        "        - Update **q_table[cur_x, cur_y, action] = (1 - alpha)*q_table[cur_x, cur_y, action] + alpha*(reward + gamma*np.max(q_table[next_x, next_y]))**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe1068f0",
      "metadata": {
        "id": "fe1068f0"
      },
      "outputs": [],
      "source": [
        "field = Field()\n",
        "q_table = np.zeros((len(field.states), len(field.states[0]), 4))\n",
        "\n",
        "alpha = .5\n",
        "epsilon = .5\n",
        "gamma = .5\n",
        "\n",
        "for _ in range(10000):\n",
        "    field = Field()\n",
        "    while not field.done():\n",
        "        actions = field.get_possible_actions()\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.choice(actions)\n",
        "        else:\n",
        "            action = np.argmax(q_table[field.state])\n",
        "        \n",
        "        cur_x, cur_y = field.state\n",
        "        (next_x, next_y), reward = field.update_next_state(action)\n",
        "        q_table[cur_x, cur_y, action] = (1 - alpha)*q_table[cur_x, cur_y, action] + alpha*(reward + gamma*np.max(q_table[next_x, next_y]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "def3b371",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "def3b371",
        "outputId": "72840207-6009-4595-cfd9-7f2bca6c3abf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[  0.      ,   0.      ,   0.      ,   0.      ],\n",
              "        [ -1.      ,   0.      ,   0.      ,   0.      ],\n",
              "        [  0.      , -10.      ,   0.      ,   0.      ],\n",
              "        [  0.      ,   0.03125 ,   0.      ,   0.03125 ],\n",
              "        [  0.015625,   0.0625  ,   0.      ,   0.0625  ],\n",
              "        [  0.03125 ,   0.125   ,   0.      ,   0.125   ],\n",
              "        [  0.0625  ,   0.25    ,   0.      ,   0.25    ],\n",
              "        [  0.125   ,   0.5     ,   0.      ,   0.5     ],\n",
              "        [  0.25    ,   0.25    ,   0.      ,   1.      ],\n",
              "        [  0.5     ,   0.125   ,   0.      ,   0.5     ],\n",
              "        [  0.25    ,   0.0625  ,   0.      ,   0.25    ],\n",
              "        [  0.125   ,   0.      ,   0.      ,   0.125   ]],\n",
              "\n",
              "       [[  0.      ,   0.      ,   0.      ,   0.      ],\n",
              "        [ -1.      ,   0.      ,   0.      ,   0.      ],\n",
              "        [  0.      , -10.      ,   0.      ,   0.      ],\n",
              "        [  0.      ,   0.0625  ,   0.015625,   0.015625],\n",
              "        [  0.03125 ,   0.125   ,   0.03125 ,   0.03125 ],\n",
              "        [  0.0625  ,   0.25    ,   0.0625  ,   0.0625  ],\n",
              "        [  0.125   ,   0.5     ,   0.125   ,   0.125   ],\n",
              "        [  0.25    ,   1.      ,   0.25    ,   0.25    ],\n",
              "        [  0.      ,   0.      ,   0.      ,   0.      ],\n",
              "        [  1.      ,   0.25    ,   0.25    ,   0.25    ],\n",
              "        [  0.5     ,   0.125   ,   0.125   ,   0.125   ],\n",
              "        [  0.25    ,   0.      ,   0.0625  ,   0.0625  ]],\n",
              "\n",
              "       [[ -5.      ,   0.      ,  -1.      ,   0.      ],\n",
              "        [  0.      ,   0.      ,   0.      ,   0.      ],\n",
              "        [  0.      , -10.      ,   0.      ,   0.      ],\n",
              "        [  0.      ,   0.03125 ,   0.03125 ,   0.      ],\n",
              "        [  0.015625,   0.0625  ,   0.0625  ,   0.      ],\n",
              "        [  0.03125 ,   0.125   ,   0.125   ,   0.      ],\n",
              "        [  0.0625  ,   0.25    ,   0.25    ,   0.      ],\n",
              "        [  0.125   ,   0.5     ,   0.5     ,   0.      ],\n",
              "        [  0.25    ,   0.25    ,   1.      ,   0.      ],\n",
              "        [  0.5     ,   0.125   ,   0.5     ,   0.      ],\n",
              "        [  0.25    ,   0.0625  ,   0.25    ,   0.      ],\n",
              "        [  0.125   ,   0.      ,   0.125   ,   0.      ]]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "q_table"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83d9654e",
      "metadata": {
        "id": "83d9654e"
      },
      "source": [
        "### Step 4: Solve a task\n",
        "- To see the path make a variable **path = np.zeros((3, 11))**\n",
        "- Create a field **Field()**\n",
        "- To count steps assign **steps = 1**\n",
        "- Assign the start state in the path to **np.nan**.\n",
        "- The we begin: while not solved.\n",
        "    - Get the **action** to take\n",
        "    - Get the next **state**\n",
        "    - Update **path** with **steps**\n",
        "    - Increment **steps** with one\n",
        "- see the **path**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd966afa",
      "metadata": {
        "id": "fd966afa"
      },
      "outputs": [],
      "source": [
        "path = np.zeros((3, 11))\n",
        "field = Field()\n",
        "steps = 1\n",
        "path[field.state[0]][field.state[1]] = np.nan\n",
        "\n",
        "while not field.done():\n",
        "    action = np.argmax(q_table[field.state])\n",
        "\n",
        "    (next_x, next_y), _ = field.update_next_state(action)\n",
        "    path[next_x][next_y] = steps\n",
        "    steps +=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4ae142e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4ae142e",
        "outputId": "8cdca0c8-f642-4a26-daf1-7d3571a094b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
              "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  3.,  0.,  0.],\n",
              "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  1., nan]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "021fd13f",
      "metadata": {
        "id": "021fd13f"
      },
      "source": [
        "> ### Note\n",
        "> - The training phase (Step 3) could just take random actions\n",
        "> - Our example (Step 4) does not learn anything new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81d50a73",
      "metadata": {
        "id": "81d50a73"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}